{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/pd1h_8px20b3__mhlx7_s2yc0000gn/T/ipykernel_17746/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK imports and downloads completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Add these imports at the TOP of your code (or right before using lemmatize_text)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')  # Optional but recommended for better lemmatization\n",
    "\n",
    "print(\"NLTK imports and downloads completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/mac/IronHacks/W7/D1/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Primeira mensagem:\n",
      "DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL I AM MIKE CHUKWU , THE MANAGER, BILLS AND EXCHANGE AT THE FOREIGN REMITTANCE DEPARTMENT OF THE ZENITH INTERNATIONAL BANK PLC. I AM WRITING THIS LETTER TO ASK FOR YOUR SUPPORT AND COOPERATION TO CARRY OUT THIS BUSINESS OPPORTUNITY IN MY DEPARTMENT. WE DISCOVERED AN ABANDONED SUM OF $15,000,000.00 (FIFTEEN MILLION UNITED STATES DOLLARS ONLY) IN AN ACCOUNT THAT BELONGS TO ONE OF OUR FOREIGN CUSTOMERS WHO DIED ALONG WITH HIS ENTIRE FAMILY OF A WIFE AND \n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(\"\\nFirst Message:\")\n",
    "print(data['text'].iloc[0][:500])  # Mostra os primeiros 500 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "train_data = pd.read_csv(\"/Users/mac/IronHacks/W7/D1/lab-natural-language-processing/data/kg_train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/mac/IronHacks/W7/D1/lab-natural-language-processing/data/kg_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# First we remove inline JavaScript/CSS\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_JS_CSS(text):\n",
    "# Remove inline CSS styles: finds any <style> tag and all content until closing </style> tag\n",
    "\n",
    "    text = re.sub(r'<style.*?>.*?</style>',             # Regex pattern to match <style> tags and their content\n",
    "                    '',                                 # Replace matched content with empty string (remove it)\n",
    "                    text,                               # The input text to process\n",
    "                    flags=re.DOTALL | re.IGNORECASE     # Flags: DOTALL makes . match newlines, IGNORECASE handles case variations\n",
    "                    )\n",
    "        \n",
    "    # Remove <style>...</style> tags and their content (duplicate operation - same as above but on variable 'texto')\n",
    "    \n",
    "    texto = re.sub(r'<style.*?>.*?</style>',            # Same regex pattern to match style tags\n",
    "                    '',                                 # Replace with empty string to remove\n",
    "                    texto,                              # Different variable name, same operation\n",
    "                    flags=re.DOTALL | re.IGNORECASE     # Same flags for multiline and case-insensitive matching\n",
    "    )\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "\n",
    "def remove_coments_html(text):\n",
    "\n",
    "    # Remove <!-- ... --> \n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we can remove the remaining tags\n",
    "\n",
    "def remover_tags_html(text):\n",
    " \n",
    "    # Remove any HTML tag\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def clean_text_complete(text):\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove prefixed 'b' (common in encoding)\n",
    "    text = re.sub(\n",
    "        r'^b[\\'\\\"]',    # Pattern: 'b' at start + quote\n",
    "        '',             # Replace with empty string\n",
    "        text            # Input text\n",
    "    )\n",
    "    \n",
    "    # 3. Remove special characters\n",
    "    text = re.sub(\n",
    "        r'[^a-zA-Z0-9\\s]',  # Pattern: keep only alphanumeric and spaces\n",
    "        '',                 # Replace with empty string\n",
    "        text                # Input text\n",
    "    )\n",
    "    \n",
    "    # 4. Remove numbers\n",
    "    text = re.sub(\n",
    "        r'\\d+',         # Pattern: one or more digits\n",
    "        '',             # Replace with empty string\n",
    "        text            # Input text\n",
    "    )\n",
    "    \n",
    "    # 5. Remove all single characters\n",
    "    text = re.sub(\n",
    "        r'\\s+[a-zA-Z]\\s+',  # Pattern: space + single letter + space\n",
    "        ' ',                # Replace with single space\n",
    "        text                # Input text\n",
    "    )\n",
    "    \n",
    "    # 6. Remove single characters from the start\n",
    "    text = re.sub(\n",
    "        r'^[a-zA-Z]\\s+',    # Pattern: single letter at start + spaces\n",
    "        '',                 # Replace with empty string\n",
    "        text                # Input text\n",
    "    )\n",
    "    \n",
    "    # 7. Substitute multiple spaces with single space\n",
    "    text = re.sub(\n",
    "        r'\\s+',         # Pattern: one or more whitespace characters\n",
    "        ' ',            # Replace with single space\n",
    "        text            # Input text\n",
    "    )\n",
    "    \n",
    "    return text.strip()  # Final cleanup - remove extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    # Get english stopwords list\n",
    "    stop_words = set(stopwords.words('english'))            #Load English stopwords\n",
    "    \n",
    "    # Split text ino words\n",
    "    words = text.split()                                    #convert text to list of words\n",
    "    \n",
    "    # Create empty list for filtered words\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    # Loop through each word and append if not stopword\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    # Join words vacl into text\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()        #Create Lemmatizer Object\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)              #Input text to tokenize\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(\n",
    "            word  # Current word to lemmatize\n",
    "        ) \n",
    "        for word in words  # For each word in the list\n",
    "    ]\n",
    "    \n",
    "    # Join words back into text\n",
    "    lemmatized_text = ' '.join(\n",
    "        lemmatized_words  # Join lemmatized words with spaces\n",
    "    )\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP 10 WORDS IN HAM MESSAGES ===\n",
      "the: 1593 times\n",
      "to: 1039 times\n",
      "of: 782 times\n",
      "and: 781 times\n",
      "a: 591 times\n",
      "in: 540 times\n",
      "that: 371 times\n",
      "is: 353 times\n",
      "for: 344 times\n",
      "on: 283 times\n",
      "\n",
      "=== TOP 10 WORDS IN SPAM MESSAGES ===\n",
      "the: 5676 times\n",
      "to: 4688 times\n",
      "of: 4118 times\n",
      "and: 3234 times\n",
      "in: 2672 times\n",
      "I: 2633 times\n",
      "you: 2273 times\n",
      "this: 2010 times\n",
      "a: 1939 times\n",
      "for: 1685 times\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_words_by_label(dataframe, text_column, label_column, top_n=10):\n",
    "    \"\"\"\n",
    "    Get top N words for each label in the dataset\n",
    "    \"\"\"\n",
    "    # Separate data by label\n",
    "    ham_messages = dataframe[\n",
    "        dataframe[label_column] == 0  # Filter rows where label is 0 (ham)\n",
    "    ]\n",
    "    \n",
    "    spam_messages = dataframe[\n",
    "        dataframe[label_column] == 1  # Filter rows where label is 1 (spam)\n",
    "    ]\n",
    "    \n",
    "    # Extract all words from ham messages\n",
    "    ham_words = []\n",
    "    for message in ham_messages[text_column]:  # For each ham message\n",
    "        words = message.split()                # Split into words\n",
    "        ham_words.extend(words)                # Add to ham words list\n",
    "    \n",
    "    # Extract all words from spam messages  \n",
    "    spam_words = []\n",
    "    for message in spam_messages[text_column]:  # For each spam message\n",
    "        words = message.split()                 # Split into words\n",
    "        spam_words.extend(words)                # Add to spam words list\n",
    "    \n",
    "    # Count word frequencies for ham\n",
    "    ham_word_counts = Counter(\n",
    "        ham_words  # Count occurrences of each word\n",
    "    )\n",
    "    \n",
    "    # Count word frequencies for spam\n",
    "    spam_word_counts = Counter(\n",
    "        spam_words  # Count occurrences of each word\n",
    "    )\n",
    "    \n",
    "    # Get top N words for each category\n",
    "    top_ham_words = ham_word_counts.most_common(\n",
    "        top_n  # Get top N most common words\n",
    "    )\n",
    "    \n",
    "    top_spam_words = spam_word_counts.most_common(\n",
    "        top_n  # Get top N most common words\n",
    "    )\n",
    "    \n",
    "    return top_ham_words, top_spam_words\n",
    "\n",
    "# üß™ GET TOP 10 WORDS FOR HAM AND SPAM\n",
    "top_ham, top_spam = get_top_words_by_label(\n",
    "    dataframe=data,           # Your dataframe\n",
    "    text_column='text',       # Column with text messages\n",
    "    label_column='label',     # Column with labels (0=ham, 1=spam)\n",
    "    top_n=10                 # Get top 10 words\n",
    ")\n",
    "\n",
    "# üìà DISPLAY RESULTS\n",
    "print(\"=== TOP 10 WORDS IN HAM MESSAGES ===\")\n",
    "for word, count in top_ham:\n",
    "    print(f\"{word}: {count} times\")\n",
    "\n",
    "print(\"\\n=== TOP 10 WORDS IN SPAM MESSAGES ===\")\n",
    "for word, count in top_spam:\n",
    "    print(f\"{word}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_text column created!\n",
      "Sample: dear sir strictly private business proposal mike chukwu manager bill exchange foreign remittance department zenith international bank plc writing letter ask support cooperation carry business opportunity department discovered abandoned sum fifteen million united state dollar account belongs one foreign customer died along entire family wife two child november plane crash since heard death expecting nextofkin come put claim money heirbecause can not release fund account unless someone applies claim nextofkin deceased indicated banking guideline unfortunately neither family member distant relative ever appeared claim said fund upon discoveryi official department agreed make business release total amount account heir fund since one came discovered maintained account bank otherwise fund returned bank treasury unclaimed fund agreed ratio sharing stated thus foreign partner u official department settlement local foreign expences incurred u course business upon successful completion transfer one colleague come country mind share intend import agricultural machinery country way recycling fund commence transaction require immediately indicate interest return email enclose private contact telephone number fax number full name address designated bank coordinate enable u file letter claim appropriate department necessary approval transfer made note also transaction must kept strictly confidential nature look forward receiving prompt response regard mr mike chukwu zenith international bank plc\n"
     ]
    }
   ],
   "source": [
    "# Apply your existing functions in sequence\n",
    "data_train['preprocessed_text'] = data_train['text'].apply(clean_text_complete)\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(lemmatize_text)\n",
    "\n",
    "data_val['preprocessed_text'] = data_val['text'].apply(clean_text_complete)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(lemmatize_text)\n",
    "\n",
    "print(\"preprocessed_text column created!\")\n",
    "print(\"Sample:\", data_train['preprocessed_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>noracheryl emailed dozen memo haiti weekend pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                   preprocessed_text  money_mark  \\\n",
       "0  dear sir strictly private business proposal mi...           1   \n",
       "1                                                              0   \n",
       "2  noracheryl emailed dozen memo haiti weekend pl...           0   \n",
       "3  dear sirfmadamc know proposal might surprise e...           1   \n",
       "4                                                fyi           0   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1505  \n",
       "1                 0         0  \n",
       "2                 0       110  \n",
       "3                 1      1382  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "\n",
    "data_train = train_data\n",
    "data_val = test_data\n",
    "\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"‚Ç¨\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Bag of Words with CountVectorizer...\n",
      "=== BAG OF WORDS RESULTS ===\n",
      "Training data shape: (5964, 1000)\n",
      "Validation data shape: (5964, 1000)\n",
      "Number of features (vocabulary size): 1000\n",
      "First 20 features: ['abacha' 'abandoned' 'abandoned sum' 'abidjan' 'able' 'abroad' 'ac'\n",
      " 'accept' 'acceptance' 'access' 'according' 'according percentage'\n",
      " 'account' 'act' 'action' 'actual' 'actually' 'address' 'administration'\n",
      " 'advice']\n",
      "\n",
      "=== SAMPLE VECTORIZED DATA ===\n",
      "Original text sample:\n",
      "dear sir strictly private business proposal mike chukwu manager bill exchange foreign remittance dep...\n",
      "\n",
      "Vectorized representation (first 10 features):\n",
      "  abandoned: 1\n",
      "  abandoned sum: 1\n",
      "\n",
      "=== BAG OF WORDS DATAFRAME (first 5 rows, first 10 features) ===\n",
      "   abacha  abandoned  abandoned sum  abidjan  able  abroad  ac  accept  \\\n",
      "0       0          1              1        0     0       0   0       0   \n",
      "1       0          0              0        0     0       0   0       0   \n",
      "2       0          0              0        0     0       0   0       0   \n",
      "3       0          0              0        0     0       0   0       1   \n",
      "4       0          0              0        0     0       0   0       0   \n",
      "\n",
      "   acceptance  access  \n",
      "0           0       0  \n",
      "1           0       0  \n",
      "2           0       0  \n",
      "3           1       0  \n",
      "4           0       0  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bag_of_words_with_count_vectorizer(texts):\n",
    "    \"\"\"\n",
    "    Implement Bag of Words using CountVectorizer\n",
    "    \"\"\"\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=1000,           # Keep only top 1000 most frequent words\n",
    "        stop_words='english',        # Remove English stopwords\n",
    "        ngram_range=(1, 2)          # Use single words and word pairs\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the texts\n",
    "    X = vectorizer.fit_transform(\n",
    "        texts  # Input texts to vectorize\n",
    "    )\n",
    "    \n",
    "    # Get feature names (vocabulary)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return X, feature_names, vectorizer\n",
    "\n",
    "# üß™ APPLY TO OUR PREPROCESSED DATA\n",
    "print(\"Applying Bag of Words with CountVectorizer...\")\n",
    "\n",
    "# Use preprocessed_text for better results\n",
    "X_train_bow, feature_names, bow_vectorizer = bag_of_words_with_count_vectorizer(\n",
    "    data_train['preprocessed_text']  # Use cleaned text\n",
    ")\n",
    "\n",
    "X_val_bow = bow_vectorizer.transform(\n",
    "    data_val['preprocessed_text']    # Use same vocabulary for validation\n",
    ")\n",
    "\n",
    "# üìä DISPLAY RESULTS\n",
    "print(\"=== BAG OF WORDS RESULTS ===\")\n",
    "print(f\"Training data shape: {X_train_bow.shape}\")\n",
    "print(f\"Validation data shape: {X_val_bow.shape}\")\n",
    "print(f\"Number of features (vocabulary size): {len(feature_names)}\")\n",
    "print(f\"First 20 features: {feature_names[:20]}\")\n",
    "\n",
    "# Show a sample of the vectorized data\n",
    "print(f\"\\n=== SAMPLE VECTORIZED DATA ===\")\n",
    "print(\"Original text sample:\")\n",
    "print(data_train['preprocessed_text'].iloc[0][:100] + \"...\")\n",
    "\n",
    "print(\"\\nVectorized representation (first 10 features):\")\n",
    "sample_vector = X_train_bow[0].toarray()[0][:10]\n",
    "for i, value in enumerate(sample_vector):\n",
    "    if value > 0:\n",
    "        print(f\"  {feature_names[i]}: {value}\")\n",
    "\n",
    "# üéØ CREATE DATAFRAME FOR BETTER VISUALIZATION (optional)\n",
    "bow_df = pd.DataFrame(\n",
    "    X_train_bow.toarray(),          # Convert sparse matrix to dense\n",
    "    columns=feature_names           # Use feature names as columns\n",
    ")\n",
    "\n",
    "print(f\"\\n=== BAG OF WORDS DATAFRAME (first 5 rows, first 10 features) ===\")\n",
    "print(bow_df.iloc[:5, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data_train into training and validation sets...\n",
      "Training set: 4771 samples\n",
      "Validation set: 1193 samples\n",
      "Spam ratio in training: 0.432\n",
      "Spam ratio in validation: 0.433\n",
      "\n",
      "Applying Bag of Words to split data...\n",
      "Training BOW shape: (4771, 1000)\n",
      "Validation BOW shape: (1193, 1000)\n",
      "\n",
      "Training classifier with proper validation set...\n",
      "Training Bag of Words Classifier...\n",
      "=== BAG OF WORDS CLASSIFIER RESULTS ===\n",
      "Accuracy: 0.9472 (94.72%)\n",
      "\n",
      "=== CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.94      0.96      0.95       677\n",
      "        Spam       0.95      0.92      0.94       516\n",
      "\n",
      "    accuracy                           0.95      1193\n",
      "   macro avg       0.95      0.94      0.95      1193\n",
      "weighted avg       0.95      0.95      0.95      1193\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "[[653  24]\n",
      " [ 39 477]]\n",
      "\n",
      "=== COMBINING BOW WITH EXTRA FEATURES ===\n",
      "Combined features shape - Train: (4771, 1003)\n",
      "Combined features shape - Val: (1193, 1003)\n",
      "\n",
      "Training classifier with BOW + Extra Features...\n",
      "Training BOW + Extra Features Classifier...\n",
      "=== BOW + EXTRA FEATURES CLASSIFIER RESULTS ===\n",
      "Accuracy: 0.9489 (94.89%)\n",
      "\n",
      "=== CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.94      0.97      0.96       677\n",
      "        Spam       0.96      0.92      0.94       516\n",
      "\n",
      "    accuracy                           0.95      1193\n",
      "   macro avg       0.95      0.95      0.95      1193\n",
      "weighted avg       0.95      0.95      0.95      1193\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "[[659  18]\n",
      " [ 43 473]]\n",
      "\n",
      "=== PERFORMANCE COMPARISON ===\n",
      "Bag of Words Only: 0.9472 (94.72%)\n",
      "BOW + Extra Features: 0.9489 (94.89%)\n"
     ]
    }
   ],
   "source": [
    "# Yor code\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Since data_val doesn't have labels, let's split data_train for training and validation\n",
    "print(\"Splitting data_train into training and validation sets...\")\n",
    "\n",
    "# Split data_train into train and validation (80% train, 20% validation)\n",
    "X_temp = data_train.drop('label', axis=1)  # Features\n",
    "y_temp = data_train['label']               # Labels\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_temp  # Keep same distribution of spam/ham\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_split)} samples\")\n",
    "print(f\"Validation set: {len(X_val_split)} samples\")\n",
    "print(f\"Spam ratio in training: {y_train_split.mean():.3f}\")\n",
    "print(f\"Spam ratio in validation: {y_val_split.mean():.3f}\")\n",
    "\n",
    "# üéØ NOW APPLY BAG OF WORDS TO THE SPLIT DATA\n",
    "print(\"\\nApplying Bag of Words to split data...\")\n",
    "\n",
    "# Fit CountVectorizer on training split and transform both\n",
    "X_train_bow_split, feature_names, bow_vectorizer = bag_of_words_with_count_vectorizer(\n",
    "    X_train_split['preprocessed_text']  # Training texts\n",
    ")\n",
    "\n",
    "X_val_bow_split = bow_vectorizer.transform(\n",
    "    X_val_split['preprocessed_text']    # Validation texts\n",
    ")\n",
    "\n",
    "print(f\"Training BOW shape: {X_train_bow_split.shape}\")\n",
    "print(f\"Validation BOW shape: {X_val_bow_split.shape}\")\n",
    "\n",
    "# üöÄ TRAIN CLASSIFIER WITH PROPER VALIDATION\n",
    "print(\"\\nTraining classifier with proper validation set...\")\n",
    "\n",
    "classifier_bow, predictions_bow, accuracy_bow = train_and_evaluate_classifier(\n",
    "    X_train=X_train_bow_split,      # Training features from split\n",
    "    X_val=X_val_bow_split,          # Validation features from split\n",
    "    y_train=y_train_split,          # Training labels from split\n",
    "    y_val=y_val_split,              # Validation labels from split\n",
    "    classifier_name=\"Bag of Words Classifier\"\n",
    ")\n",
    "\n",
    "# üí° EXTRA: COMBINE WITH YOUR EXTRA FEATURES\n",
    "print(\"\\n=== COMBINING BOW WITH EXTRA FEATURES ===\")\n",
    "\n",
    "# Get the extra features from the split data\n",
    "extra_features_train = X_train_split[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "extra_features_val = X_val_split[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "# Combine BOW with extra features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train_combined = hstack([X_train_bow_split, extra_features_train])\n",
    "X_val_combined = hstack([X_val_bow_split, extra_features_val])\n",
    "\n",
    "print(f\"Combined features shape - Train: {X_train_combined.shape}\")\n",
    "print(f\"Combined features shape - Val: {X_val_combined.shape}\")\n",
    "\n",
    "# Train classifier with combined features\n",
    "print(\"\\nTraining classifier with BOW + Extra Features...\")\n",
    "classifier_combined, predictions_combined, accuracy_combined = train_and_evaluate_classifier(\n",
    "    X_train=X_train_combined,      # Combined features\n",
    "    X_val=X_val_combined,          # Combined features\n",
    "    y_train=y_train_split,         # Training labels\n",
    "    y_val=y_val_split,             # Validation labels\n",
    "    classifier_name=\"BOW + Extra Features Classifier\"\n",
    ")\n",
    "\n",
    "# üìä COMPARE PERFORMANCE\n",
    "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"Bag of Words Only: {accuracy_bow:.4f} ({accuracy_bow*100:.2f}%)\")\n",
    "print(f\"BOW + Extra Features: {accuracy_combined:.4f} ({accuracy_combined*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BAG OF WORDS ONLY ===\n",
      "Bag of Words: 0.9472 (94.72%)\n",
      "\n",
      "=== TF-IDF ONLY ===\n",
      "TF-IDF: 0.9581 (95.81%)\n",
      "\n",
      "=== BAG OF WORDS + EXTRA FEATURES ===\n",
      "BOW + Extra Features: 0.9489 (94.89%)\n",
      "\n",
      "=== TF-IDF + EXTRA FEATURES ===\n",
      "TF-IDF + Extra Features: 0.9581 (95.81%)\n",
      "\n",
      "==================================================\n",
      "FEATURE SET COMPARISON SUMMARY\n",
      "==================================================\n",
      "\n",
      "RANKED BY PERFORMANCE:\n",
      "1. TF-IDF: 0.9581 (95.81%)\n",
      "2. TF-IDF + Extra Features: 0.9581 (95.81%)\n",
      "3. BOW + Extra Features: 0.9489 (94.89%)\n",
      "4. Bag of Words: 0.9472 (94.72%)\n",
      "\n",
      "==================================================\n",
      "TRAINING FINAL MODEL WITH BEST FEATURES\n",
      "==================================================\n",
      "Best performing feature set: TF-IDF\n",
      "Final Model Accuracy: 0.9581 (95.81%)\n",
      "\n",
      "==================================================\n",
      "PREPARING KAGGLE SUBMISSION\n",
      "==================================================\n",
      "Test predictions ready: 5964 samples\n",
      "Spam predictions: 2493 (41.80%)\n",
      "Ham predictions: 3471\n",
      "Submission file saved: kaggle_submission.csv\n",
      "\n",
      "==================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==================================================\n",
      "Top 15 features most predictive of SPAM:\n",
      "  transaction: 3.5839\n",
      "  bank: 3.5266\n",
      "  account: 3.5165\n",
      "  fund: 3.5072\n",
      "  kin: 3.4408\n",
      "  money: 3.4003\n",
      "  sum: 3.3340\n",
      "  transfer: 3.3007\n",
      "  foreigner: 2.8506\n",
      "  nigeria: 2.7123\n",
      "  dollar: 2.7012\n",
      "  deposited: 2.6858\n",
      "  security company: 2.6533\n",
      "  god: 2.6532\n",
      "  reply: 2.6397\n",
      "\n",
      "Top 15 features most predictive of HAM:\n",
      "  fyi: -6.2690\n",
      "  cheryl: -4.8694\n",
      "  huma: -4.7980\n",
      "  ok: -4.7717\n",
      "  tomorrow: -4.7057\n",
      "  pm: -4.5872\n",
      "  hillary: -4.3388\n",
      "  speech: -4.2386\n",
      "  pls: -3.9153\n",
      "  morning: -3.7350\n",
      "  vote: -3.7129\n",
      "  obama: -3.6644\n",
      "  talk: -3.6049\n",
      "  night: -3.5369\n",
      "  benghazi: -3.4754\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the classifier (FIXED - cannot be changed)\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "def evaluate_feature_set(X_train, X_val, y_train, y_val, feature_name):\n",
    "    \"\"\"\n",
    "    Evaluate a specific feature set with the fixed classifier\n",
    "    \"\"\"\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"{feature_name}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    return accuracy\n",
    "\n",
    "# üéØ FEATURE SET 1: BAG OF WORDS ONLY\n",
    "print(\"=== BAG OF WORDS ONLY ===\")\n",
    "X_train_bow, bow_features, bow_vectorizer = bag_of_words_with_count_vectorizer(X_train_split['preprocessed_text'])\n",
    "X_val_bow = bow_vectorizer.transform(X_val_split['preprocessed_text'])\n",
    "accuracy_bow = evaluate_feature_set(X_train_bow, X_val_bow, y_train_split, y_val_split, \"Bag of Words\")\n",
    "\n",
    "# üéØ FEATURE SET 2: TF-IDF ONLY\n",
    "print(\"\\n=== TF-IDF ONLY ===\")\n",
    "def create_tfidf_features(texts_train, texts_val):\n",
    "    \"\"\"Create TF-IDF features\"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(texts_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(texts_val)\n",
    "    return X_train_tfidf, X_val_tfidf, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, X_val_tfidf, tfidf_vectorizer = create_tfidf_features(\n",
    "    X_train_split['preprocessed_text'], \n",
    "    X_val_split['preprocessed_text']\n",
    ")\n",
    "accuracy_tfidf = evaluate_feature_set(X_train_tfidf, X_val_tfidf, y_train_split, y_val_split, \"TF-IDF\")\n",
    "\n",
    "# üéØ FEATURE SET 3: BAG OF WORDS + EXTRA FEATURES\n",
    "print(\"\\n=== BAG OF WORDS + EXTRA FEATURES ===\")\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Get extra features\n",
    "extra_features_train = X_train_split[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "extra_features_val = X_val_split[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "# Combine BOW with extra features\n",
    "X_train_bow_extra = hstack([X_train_bow, extra_features_train])\n",
    "X_val_bow_extra = hstack([X_val_bow, extra_features_val])\n",
    "accuracy_bow_extra = evaluate_feature_set(X_train_bow_extra, X_val_bow_extra, y_train_split, y_val_split, \"BOW + Extra Features\")\n",
    "\n",
    "# üéØ FEATURE SET 4: TF-IDF + EXTRA FEATURES\n",
    "print(\"\\n=== TF-IDF + EXTRA FEATURES ===\")\n",
    "X_train_tfidf_extra = hstack([X_train_tfidf, extra_features_train])\n",
    "X_val_tfidf_extra = hstack([X_val_tfidf, extra_features_val])\n",
    "accuracy_tfidf_extra = evaluate_feature_set(X_train_tfidf_extra, X_val_tfidf_extra, y_train_split, y_val_split, \"TF-IDF + Extra Features\")\n",
    "\n",
    "# üìä COMPARISON SUMMARY\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE SET COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = {\n",
    "    \"Bag of Words\": accuracy_bow,\n",
    "    \"TF-IDF\": accuracy_tfidf,\n",
    "    \"BOW + Extra Features\": accuracy_bow_extra,\n",
    "    \"TF-IDF + Extra Features\": accuracy_tfidf_extra\n",
    "}\n",
    "\n",
    "# Sort by performance\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRANKED BY PERFORMANCE:\")\n",
    "for i, (name, acc) in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {name}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "# üèÜ TRAIN FINAL MODEL WITH BEST FEATURES\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING FINAL MODEL WITH BEST FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_feature_name = sorted_results[0][0]\n",
    "print(f\"Best performing feature set: {best_feature_name}\")\n",
    "\n",
    "if best_feature_name == \"Bag of Words\":\n",
    "    X_final_train = X_train_bow\n",
    "    X_final_val = X_val_bow\n",
    "    final_vectorizer = bow_vectorizer\n",
    "elif best_feature_name == \"TF-IDF\":\n",
    "    X_final_train = X_train_tfidf\n",
    "    X_final_val = X_val_tfidf\n",
    "    final_vectorizer = tfidf_vectorizer\n",
    "elif best_feature_name == \"BOW + Extra Features\":\n",
    "    X_final_train = X_train_bow_extra\n",
    "    X_final_val = X_val_bow_extra\n",
    "    final_vectorizer = bow_vectorizer\n",
    "else:  # TF-IDF + Extra Features\n",
    "    X_final_train = X_train_tfidf_extra\n",
    "    X_final_val = X_val_tfidf_extra\n",
    "    final_vectorizer = tfidf_vectorizer\n",
    "\n",
    "# Train final model on entire training split\n",
    "final_classifier = MultinomialNB()\n",
    "final_classifier.fit(X_final_train, y_train_split)\n",
    "\n",
    "# Final validation performance\n",
    "final_predictions = final_classifier.predict(X_final_val)\n",
    "final_accuracy = accuracy_score(y_val_split, final_predictions)\n",
    "\n",
    "print(f\"Final Model Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "\n",
    "# üéØ PREPARE KAGGLE SUBMISSION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPARING KAGGLE SUBMISSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare test data (data_val) using the same preprocessing\n",
    "if best_feature_name in [\"Bag of Words\", \"BOW + Extra Features\"]:\n",
    "    X_test = bow_vectorizer.transform(data_val['preprocessed_text'])\n",
    "else:\n",
    "    X_test = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Add extra features if needed\n",
    "if best_feature_name in [\"BOW + Extra Features\", \"TF-IDF + Extra Features\"]:\n",
    "    test_extra_features = data_val[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "    X_test = hstack([X_test, test_extra_features])\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_classifier.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'text': data_val['text'],\n",
    "    'label': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"Test predictions ready: {len(test_predictions)} samples\")\n",
    "print(f\"Spam predictions: {test_predictions.sum()} ({test_predictions.mean()*100:.2f}%)\")\n",
    "print(f\"Ham predictions: {len(test_predictions) - test_predictions.sum()}\")\n",
    "\n",
    "# Save submission file\n",
    "submission_file = 'kaggle_submission.csv'\n",
    "submission[['text', 'label']].to_csv(submission_file, index=False)\n",
    "print(f\"Submission file saved: {submission_file}\")\n",
    "\n",
    "# üìà FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if best_feature_name in [\"Bag of Words\", \"BOW + Extra Features\"]:\n",
    "    feature_names = bow_vectorizer.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Add extra feature names if used\n",
    "if best_feature_name in [\"BOW + Extra Features\", \"TF-IDF + Extra Features\"]:\n",
    "    feature_names = np.append(feature_names, ['money_mark', 'suspicious_words', 'text_len'])\n",
    "\n",
    "# Get feature importance (log probabilities difference)\n",
    "spam_probs = final_classifier.feature_log_prob_[1]\n",
    "ham_probs = final_classifier.feature_log_prob_[0]\n",
    "importance_scores = spam_probs - ham_probs\n",
    "\n",
    "print(\"Top 15 features most predictive of SPAM:\")\n",
    "top_spam_indices = importance_scores.argsort()[-15:][::-1]\n",
    "for idx in top_spam_indices:\n",
    "    if idx < len(feature_names):\n",
    "        print(f\"  {feature_names[idx]}: {importance_scores[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 features most predictive of HAM:\")\n",
    "top_ham_indices = importance_scores.argsort()[:15]\n",
    "for idx in top_ham_indices:\n",
    "    if idx < len(feature_names):\n",
    "        print(f\"  {feature_names[idx]}: {importance_scores[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
